-- rgb-d slam tutorial --

This is the code written for the tutorial on my cnblogs:
    http://www.cnblogs.com/gaoxiang12/
    Please visit this blog if you are interested.


part V depth and rgb png, can download from
https://pan.baidu.com/disk/home#list/vmode=list&path=%2F%E6%8A%80%E6%9C%AF%2Fslam

Latest update: 2017-03-04


SLAM是由“定位”（Localization)和“建图”（Mapping)两部分构成的。现在来看定位问题。要求解机器人的运动，首先要解决这样一个问题：给定了两个图像，如何知道图像的运动关系呢？

这个问题可以用基于特征的方法（feature-based）或直接的方法（direct method）来解。

我们的目的是求出一个旋转矩阵RR和位移矢量t

什么是视觉里程计呢？简而言之，就是把新来的数据与上一帧进行匹配，估计其运动，然后再把运动累加起来的东西。
这实际上和滤波器很像，通过不断的两两匹配，估计机器人当前的位姿，过去的就给丢弃了。这个思路比较简单，实际当中也比较有效，能够保证局部运动的正确性。下面我们来实现一下visual odometry。

工具函数：将cv的旋转矢量与位移矢量转换为变换矩阵，类型为Eigen::Isometry3d； 


一起做RGB-D SLAM (2) 将图像转换为点云
编写一个将图像转换为点云的程序。该程序是后期处理地图的基础。最简单的点云地图即是把不同位置的点云进行拼接得到的。
假设这个世界由一个点云来描述：X={x1,…,xn}X={x1,…,xn}. 其中每一个点呢，有 r,g,b,x,y,zr,g,b,x,y,z一共6个分量，表示它们的颜色与空间位置。颜色方面，主要由彩色图像记录； 而空间位置，可以由图像和相机模型、姿态一起计算出来。
简而言之，一个空间点[x,y,z][x,y,z]和它在图像中的像素坐标[u,v,d][u,v,d] (dd指深度数据) 的对应关系是这样的：
u=x⋅fxz+cx
u=x⋅fxz+cx
v=y⋅fyz+cy
v=y⋅fyz+cy
d=z⋅s
d=z⋅s
本讲中，我们实现了一个从2D图像到3D点云的转换程序。下一讲，我们将探讨图像的特征点提取与配准。

一起做RGB-D SLAM (3)特征提取与配准, 求出两个图像的旋转矩阵R和位移矢量t

SLAM是由“定位”（Localization)和“建图”（Mapping)两部分构成的。现在来看定位问题。要求解机器人的运动，首先要解决这样一个问题：给定了两个图像，如何知道图像的运动关系呢？

　　这个问题可以用基于特征的方法（feature-based）或直接的方法（direct method）来解。虽说直接法已经有了一定的发展，但目前主流的方法还是基于特征点的方式。在后者的方法中，首先你需要知道图像里的“特征”，以及这些特征的一一对应关系。

　　假设我们有两个帧：F1F1和F2F2. 并且，我们获得了两组一一对应的特征点：
我们的目的是求出一个旋转矩阵R和位移矢量t，使得：
求解完成后，rvec和tvec就含有了位移和旋转的信息，至此，我们已经成功地计算出两个图像的运动啦！

本节中，我们介绍了如何提取、匹配图像的特征，并通过这些匹配，使用ransac方法估计图像的运动。下一节，我们将介绍如何使用刚得到的平移、旋转向量来拼接点云。至此，我们就完成了一个只有两帧的迷你SLAM程序。

一起做RGB-D SLAM (4)  点云拼接

上一讲中，我们理解了如何利用图像中的特征点，估计相机的运动。最后，我们得到了一个旋转向量与平移向量。那么读者可能会问：这两个向量有什么用呢？在这一讲里，我们就要使用这两个向量，把两张图像的点云给拼接起来，形成更大的点云。
点云的拼接，实质上是对点云做变换的过程。这个变换往往是用变换矩阵(transform matrix)来描述的：
T=[R3×3O1×3t3×11]∈R4×4
该矩阵的左上部分是一个3×33×3的旋转矩阵，它是一个正交阵。右上部分是3×13×1的位移矢量。左下是3×13×1的缩放矢量，在SLAM中通常取成0，因为环境里的东西不太可能突然变大变小（又没有缩小灯）。右下角是个1. 这样的一个阵可以对点或者其他东西进行齐次变换：
⎡⎣⎢⎢⎢⎢y1y2y31⎤⎦⎥⎥⎥⎥=T⋅⎡⎣⎢⎢⎢⎢x1x2x31⎤⎦⎥⎥⎥⎥[y1y2y31]=T⋅[x1x2x31]
由于变换矩阵结合了旋转和缩放，是一种较为经济实用的表达方式。它在机器人和许多三维空间相关的科学中都有广泛的应用。PCL里提供了点云的变换函数，只要给定了变换矩阵，就能对移动整个点云：
pcl::transformPointCloud( input, output, T );
小萝卜：所以我们现在就是要把OpenCV里的旋转向量、位移向量转换成这个矩阵喽？
师兄：对！OpenCV认为旋转矩阵RR，虽然有3×33×3那么大，自由变量却只有三个，不够节省空间。所以在OpenCV里使用了一个向量来表达旋转。向量的方向是旋转轴，大小则是转过的弧度.
小萝卜：但是我们又把它变成了矩阵啊，这不就没有意义了吗！
师兄：呃，这个，确实如此。不管如何，我们先用罗德里格斯变换（Rodrigues）将旋转向量转换为矩阵，然后“组装”成变换矩阵。代码如下：

至此，我们已经实现了一个只有两帧的SLAM程序。然而，也许你还不知道，这已经是一个视觉里程计(Visual Odometry)啦！只要不断地把进来的数据与上一帧对比，就可以得到完整的运动轨迹以及地图了呢，但如果把每帧图像都放到地图，那么会导致地图越来越大，所以需要关键帧处理，也就是需要一个SLAM后段程序，此处用到了g2o。
我们先做视觉里程计(Visual Odometry), 然后再优化。

一起做RGB-D SLAM (5)  Visual Odometry (视觉里程计)
什么是视觉里程计呢？简而言之，就是把新来的数据与上一帧进行匹配，估计其运动，然后再把运动累加起来的东西。画成示意图的话，就是下面这个样子：这实际上和滤波器很像，通过不断的两两匹配，估计机器人当前的位姿，过去的就给丢弃了。这个思路比较简单，实际当中也比较有效，能够保证局部运动的正确性。下面我们来实现一下visual odometry。

一起做RGB-D SLAM (6)   图优化工具g2o的入门
在上一讲中，我们介绍了如何使用两两匹配，搭建一个视觉里程计。那么，这个里程计有什么不足呢？
1.  一旦出现了错误匹配，整个程序就会跑飞。
2. 误差会累积。常见的现象是：相机转过去的过程能够做对，但转回来之后则出现明显的偏差。
3. 效率方面不尽如人意。在线的点云显示比较费时。
累积误差是里程计中不可避免的，后续的相机姿态依赖着前面的姿态。想要保证地图的准确，必须要保证每次匹配都精确无误，而这是难以实现的。所以，我们希望用更好的方法来做slam。不仅仅考虑两帧的信息，而要把所有整的信息都考虑进来，成为一个全slam问题（full slam）。下图为累积误差的一个例子。右侧是原有扫过的地图，左侧是新扫的，可以看到出现了明显的不重合。
所以，我们这一讲要介绍姿态图（pose graph），它是目前视觉slam里最常用的方法之一。
不过06年之后，人们注意到slam构建的ba问题的稀疏性质，所以用稀疏的BA算法（sparse BA）求解这个图，才使BA在slam里广泛地应用起来。
为什么说slam里的BA  Bundle Adjustment(BA)问题稀疏呢？因为同样的场景很少出现在许多位置中。这导致上面的pose graph中，图GG离全图很远，只有少部分的节点存在直接边的联系。这就是姿态图的稀疏性。求解BA的软件包有很多，感兴趣的读者可以去看wiki: https://en.wikipedia.org/wiki/Bundle_adjustment。我们这里介绍的g2o（Generalized Graph Optimizer），就是近年很流行的一个图优化求解软件包。下面我们通过实例代码，帮助大家入门g2o。
顶点和边有不同的类型，这要看我们想求解什么问题。由于我们是3D的slam，所以顶点取成了相机姿态：g2o::VertexSE3，而边则是连接两个VertexSE3的边：g2o::EdgeSE3。如果你想用别的类型的顶点（如2Dslam，路标点），你可以看看/usr/local/include/g2o/types/下的文件，基本上涵盖了各种slam的应用，应该能满足你的需求。
g2o允许你使用不同的优化求解器（然而实际效果似乎差别不大）。你可以选用csparse, pcg, cholmod等等。我们这里使用csparse为例啦, g2o的优化结果是存储在一个.g2o的文本文件里的，可以使用g2o_viewer打开.g2o 文件。

一起做RGB-D SLAM(7) （完结篇） 添加回环检测
上一讲中，我们介绍了图优化软件g2o的使用。本讲，我们将实现一个简单的回环检测程序，利用g2o提升slam轨迹与地图的质量。本讲结束后，读者朋友们将得到一个完整的slam程序，可以跑通我们在百度云上给出的数据了。
上一讲的程序离完整的slam还有哪些距离。主要说来有两点：

关键帧的提取。把每一帧都拼到地图是去是不明智的。因为帧与帧之间距离很近，导致地图需要频繁更新，浪费时间与空间。所以，我们希望，当机器人的运动超过一定间隔，就增加一个“关键帧”。最后只需把关键帧拼到地图里就行了。
回环的检测。回环的本质是识别曾经到过的地方。最简单的回环检测策略，就是把新来的关键帧与之前所有的关键帧进行比较，不过这样会导致越往后，需要比较的帧越多。所以，稍微快速一点的方法是在过去的帧里随机挑选一些，与之进行比较。更进一步的，也可以用图像处理/模式识别的方法计算图像间的相似性，对相似的图像进行检测。
大体上如此，也可以作一些更改。例如在线跑的话呢，可以定时进行一次优化与拼图。或者，在成功检测到回环时，同时检测这两个帧附近的帧，那样得到的边就更多啦。再有呢，如果要做实用的程序，还要考虑机器人如何运动，如果跟丢了怎么进行恢复等一些实际的问题呢。

资源： 
1 depth/rgb png: http://yun.baidu.com/s/1i33uvw5
2 http://www.cnblogs.com/gaoxiang12/tag/%E4%B8%80%E8%B5%B7%E5%81%9ARGB-D%20SLAM/
3 http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5



